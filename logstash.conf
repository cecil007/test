input {
	file {
		path => "/mnt/ufo_logs/track_*"
		type => "track"
	}

	file {
		path => "/mnt/ufo_logs/internal_track_*"
		type => "internal"
	}

	file {
		path => "/mnt/ufo_logs/sys_*"
		type => "sys"
	}

	file {
		path => "/mnt/ufo_logs/else/**/*"
		type => "else"
	}
}


filter {
	if [type] in ["track", "internal"] {
		multiline {
			pattern => "\[\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3}\].*"
			negate => true
			what => "previous"
		}
	}
}

filter {

	# env
	mutate {
		add_field => {
			"_env" => "prod"
		}
	}

	if [type] in ["track", "internal"]
	{
		grok {
			#break_on_match => false
	                #keep_empty_captures => true
			match => {"message" => "(?m)\[%{DATETIME:tmp_timestamp}\].*RequestHeader:%{SOMETHING:RequestHeader}(\\n)?(\s)*RequestBody:%{SOMETHING:RequestBody}(\\n)?(\s)*(.*ErrorStack:%{SOMETHING:ErrorStack})*.ResponseMsg:%{SOMETHING:ResponseMsg}(\\n)?(\s)*HTTPResult:%{ANYTHING:HTTPResult}"}
		}
		grok {
			match => {"HTTPResult" => "%{NUMBER:status} %{WORD:method} %{URIPATH:uri}(:?%{URIPARAM:uri_param})? \(%{IP:server}\) %{NUMBER:duration}ms"}
		}

		# json解析
		json {
			source => "RequestHeader"
			target => "tmpRequestHeader"
		}


		json {
			source => "RequestBody"
			target => "Body"
		}

		json {
			source => "ResponseMsg"
			target => "tmpResponseMsg"
		}

		# 字段处理
		# 时间格式处理
		date {
			match => [ "tmp_timestamp" , "yyyy-MM-dd HH:mm:ss,SSS" ]
	#		timezone => "UTC"
		}

		if [ErrorStack]
		{
			mutate {
				add_field => {
					"has_error" => true
				}
			}
		} else {
			mutate {
				add_field => {
					"has_error" => false
				}
			}
		}


		mutate {
			add_field => {
				"platform" => "%{Body[common][platform]}"
				"version" => "%{Body[common][version]}"
			}
		}

		if "Cmdid" in [tmpRequestHeader] {
			mutate {
				add_field => {
					"cmdid" => "%{tmpRequestHeader[Cmdid]}"
				}
			}
		} else {
			mutate {
				add_field => {
					"cmdid" => "%{tmpRequestHeader[cmdid]}"
				}
			}
		}


		if "geo" in [Body][common] and [Body][common][geo] != "" {
                        mutate {
                                split => ["[Body][common][geo]", ","]
                        }
                        mutate {
                                 add_field => {
                                        "[geoip][location][lat]" => "%{Body[common][geo][0]}"
                                        "[geoip][location][lon]" => "%{Body[common][geo][1]}"
                                }
                        }
                }else {
                        geoip {
                                source => "tmpRequestHeader[X-Real-Ip]"
                        }
                }





		mutate {
			
			add_field => {
#				"cmdid" => "%{tmpRequestHeader[Cmdid]}"
				"userid" => "%{tmpResponseMsg[common][userid]}"
				"code" => "%{tmpResponseMsg[common][code]}"
			}
			remove_field => ["tmpRequestHeader", "tmpResponseMsg", "tmp_timestamp","[Body][params][func_args]","[Body][params][func_kwargs]"]
			convert => {
				"duration" => "float"
				"status" => "integer"
				"cmdid" => "integer"
				"userid" => "integer"
				"platform" => "integer"
				"has_error" => "boolean"
				"code" => "integer"
			}
		}
	}
}

output {


        elasticsearch {
                hosts => ["192.168.10.10", "192.168.10.11", "192.168.10.12", "192.168.10.13", "192.168.10.14"]
                index => "logstash-%{_env}-%{type}-%{+YYYY.MM.dd}"
        }
#
#	elasticsearch {
#		hosts => ["192.168.10.51"]
#		index => "logstash-%{_env}-%{type}-%{+YYYY.MM.dd}"
#	}
#	stdout { codec => rubydebug }


	kafka {
                bootstrap_servers => "192.168.10.76:9092,192.168.10.75:9092,192.168.10.74:9092"
                topic_id => "test"
        }
#	redis {
#		host => "192.168.0.8"
##		password => ""
#		db => 10
#		data_type => "channel"
#		key => "logstash"
#	}
}
